% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/loco.R
\name{loco}
\alias{loco}
\title{Variable importance via sample splitting.}
\usage{
loco(x, y, train.fun, predict.fun, active.fun, alpha = 0.1, split = NULL,
  seed = NULL, verbose = FALSE)
}
\arguments{
\item{x}{Matrix of features, of dimension (say) n x p.}

\item{y}{Vector of responses, of length (say) n.}

\item{train.fun}{A function to perform model training, i.e., to produce an
estimator of E(Y|X), the conditional expectation of the response variable
Y given features X. Its input arguments should be x: matrix of features,
and y: vector of responses.}

\item{predict.fun}{A function to perform prediction for the (mean of the)
responses at new feature values. Its input arguments should be out: output
produced by train.fun, and newx: feature values at which we want to make
predictions.}

\item{active.fun}{A function which takes the output of train.fun, and reports
which features are active for each fitted model contained in this output.
Its only input argument should be out: output produced by train.fun.}

\item{alpha}{Miscoverage level for the confidence intervals, i.e., intervals
with coverage 1-alpha are formed. Default for alpha is 0.1.}

\item{split}{Indices that define the data-split to be used (i.e., the indices
define the first half of the data-split, on which the model is trained).
Default is NULL, in which case the split is chosen randomly.}

\item{seed}{Integer to be passed to set.seed before defining the random 
data-split to be used. Default is NULL, which effectively sets no seed.
If both split and seed are passed, the former takes priority and the latter
is ignored.}

\item{verbose}{Should intermediate progress be printed out? Default is FALSE.}
}
\value{
A list with the following components: inf.z, inf.sign, inf.wilcox,
  active, master. The first three are lists, containing the results of LOCO
  inference with the Z-test, sign text, and Wilcoxon signed rank test,
  respectively. More
  details on these tests are given below. These lists have one element per
  tuning step inherent to the training and prediction functions, train.fun
  and predict.fun. The fourth returned component active is a list, with one
  element per tuning step, that reports which features are active in the
  corresponding fitted model. The last returned component master collects
  all active features across all tuning steps, for easy reference.
}
\description{
Compute confidence intervals for mean (or median) excess prediction error
  after leaving out one feature.
}
\details{
In leave-one-covariate-out or LOCO inference, the training data is
  split in two parts, and the first part is used to train a model, or some
  number of models across multiple tuning steps (e.g., indexed by different 
  tuning parameter values lambda in the lasso, or different steps along the
  forward stepwise path). In each model, each variable is left out one at
  time from the first part of the data, and the entire training procedure is
  repeated without this variable in consideration. Residuals are computed on
  both from the original fitted model, and the re-fitted model without the
  variable in consideration. A pairwise difference between the latter and
  former residuals is computed, and either a Z-test, sign test, or Wilcoxon
  signed rank test is performed to test either the mean or median difference
  here being zero. The Z-test is of course approximately valid under the
  conditions needed for the CLT; the sign test is distribution-free and exact
  in finite samples (only assumes continuity of the underlying distribution);
  the Wilcoxon signed rank test is also distribution-free and exact in finite
  samples, but may offer more power (it assumes both continuity and symmetry
  of the underlying distribution).

  A few other important notes: p-values here are from a one-sided test of the
  target parameter (mean or median excess test error) being equal to zero
  versus greater than zero. Confidence intervals are from inverting the
  two-sided version of this test. 
  Furthermore, all p-values and confidence intervals have been Bonferroni",
  corrected for multiplicity.
}
\examples{
## Use lasso + CV to choose a model, then test variable importance in the
## selected model

# Generate some example training data
set.seed(33)
n = 200; p = 500; s = 5
x = matrix(rnorm(n*p),n,p)
beta = 2*c(rnorm(s),rep(0,p-s)) 
y = x \%*\% beta + rnorm(n)

# Lasso training and prediction functions, using cross-validation over 100
# values of lambda, with the 1se rule
funs = lasso.funs(nlambda=100,cv=TRUE,cv.rule="1se")

# Split-sample LOCO analysis
out.loco = loco(x, y, alpha=0.1, train.fun=funs$train, predict.fun=funs$predict,
  active.fun=funs$active.fun, verbose=TRUE)
out.loco

# Plot the Wilcoxon intervals
ylim = range(out.loco$inf.wilcox[[1]][,2:3])
J = length(out.loco$active[[1]])
plot(c(), c(), xlim=c(1,J), ylim=ylim, xaxt="n",
     main="LOCO analysis from lasso + CV model",
     xlab="Variable", ylab="Confidence interval")
axis(side=1, at=1:J, labels=FALSE)
for (j in 1:J) {
  axis(side=1, at=j, labels=out.loco$active[[1]][j], cex.axis=0.75,
       line=0.5*j\%\%2)
}
abline(h=0)
segments(1:J, out.loco$inf.wilcox[[1]][,2],
         1:J, out.loco$inf.wilcox[[1]][,3],
         col="red", lwd=2)
}
\author{
Ryan Tibshirani, Larry Wasserman
}
\references{
"Distribution-Free Predictive Inference for Regression" by
  Max G'Sell, Jing Lei, Alessandro Rinaldo, Ryan Tibshirani, Larry Wasserman,
  http://arxiv.org/pdf/xxxx.pdf, 2016.
}

